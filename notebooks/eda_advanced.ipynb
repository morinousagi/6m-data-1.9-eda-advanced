{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Wrangling & Analysis\n",
    "\n",
    "## Lesson Overview\n",
    "\n",
    "This workshop transforms basic Python knowledge into professional data manipulation skills. We follow the \"Extract-Transform-Analyze\" workflow across 4 distinct sections:\n",
    "\n",
    "1.  **Part 1: Financial Time Series & Window Functions**\n",
    "    * Handling Datetime objects and indexing\n",
    "    * Resampling and Frequency conversion\n",
    "    * Window functions (Rolling means)\n",
    "    * Covariance and Correlation\n",
    "2.  **Part 2: Data Wrangling (Merge & Reshape)**\n",
    "    * Merging datasets (Inner, Outer, Left, Right joins)\n",
    "    * Reshaping data: Melt and Pivot\n",
    "3.  **Part 3: Aggregation & Reporting**\n",
    "    * GroupBy mechanics (Split-Apply-Combine)\n",
    "    * Pivot Tables and Cross-Tabulations\n",
    "4.  **Part 4: Advanced Toolkit (Optional/Deep Dive)**\n",
    "    * Hierarchical Indexing (MultiIndex)\n",
    "    * Concatenation\n",
    "    * Stacking/Unstacking\n",
    "    * Advanced GroupBy: Apply and Transform\n",
    "\n",
    "---\n",
    "\n",
    "**Setup:** Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Financial Time Series & Window Functions\n",
    "\n",
    "**Learning Objective:** Handle data where the *order* matters (Time Series). We will learn to convert strings to dates, handle missing business days, smoothing volatile data, and analyzing stock correlations.\n",
    "\n",
    "## 1.1 Handling Date Time Data\n",
    "\n",
    "Pandas is oriented towards working with arrays of dates, whether used as an axis index or a column.\n",
    "\n",
    "The `to_datetime` method parses may different kinds of date representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"2011-07-06 12:00:00\", \"2011-08-06 00:00:00\"]\n",
    "\n",
    "pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses `NaT` (Not a Time) as null values for datetime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.to_datetime(dates + [None])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Python uses the `datetime` module to handle date and time data. Pandas has a `Timestamp` object that is similar to the `datetime` object. \n",
    "\n",
    "If you use `datetime` objects as index to a Series or DataFrame, Pandas will automatically convert them to `DatetimeIndex` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7), \n",
    "         datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\n",
    "\n",
    "ts = pd.Series(np.random.standard_normal(6), index=dates)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other Series, arithmetic operations between differently indexed time series automatically align on the dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [::2] selects every second element\n",
    "ts + ts[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing & Slicing\n",
    "\n",
    "You can index by passing a `datetime`, `Timestamp` or `string` that is interpretable as a date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[datetime(2011, 1, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[pd.Timestamp(\"2011-01-07\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[\"2011-01-07\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even specify the year or year-month strings to slice a range of data. This is very powerful for quick analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_range generate an array of dates\n",
    "longer_ts = pd.Series(np.random.standard_normal(1000), \n",
    "                      index=pd.date_range(\"2000-01-01\", periods=1000))\n",
    "\n",
    "longer_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all data from 2001\n",
    "longer_ts[\"2001\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all data from May 2001\n",
    "longer_ts[\"2001-05\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing with strings\n",
    "longer_ts[\"2001-05\":].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Use `date_range` to generate a Series of random values from 1-31st January 2023. Then slice the Series to return data from 5-15th January."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Real-World Application: Stock Market Data\n",
    "\n",
    "Let's load stock prices (AAPL, GOOG, IBM, MSFT) and trade volumes. We use `.read_pickle` here as it preserves the datetime objects native to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_pickle(\"../data/yahoo_price.pkl\")\n",
    "volume = pd.read_pickle(\"../data/yahoo_volume.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Index\n",
    "Notice the index is a `DatetimeIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access attributes like `day_of_week` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.index.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the datetime is in a column instead of the index, you can use the `dt` accessor to access the datetime properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_reindex = price.reset_index()\n",
    "price_reindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_reindex[\"Date\"].dt.day_name().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Get the week of year from the date column and create a new column `week_of_year`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Resampling (Frequency Conversion)\n",
    "\n",
    "As you can see from above, the dates are on business days. If you want to change the frequency to calendar days (e.g. including weekends), we use `resample`.\n",
    "\n",
    "This introduces missing data (NaN) for weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('D').asfreq()\n",
    "price_resampled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Fill (`ffill`)**: If you want to fill the na values with the most recent value (common in finance - Saturday price is same as Friday close), you can use the `.ffill()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('D').ffill()\n",
    "price_resampled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to resample to a **lower frequency** (e.g. Monthly 'MS' - Month Start) you need to provide an aggregation method (like `mean`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('MS').mean()\n",
    "price_resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Resample price to `yearly` (start of year) frequency, use `sum` as aggregation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Window Functions (Moving Averages)\n",
    "\n",
    "You can apply functions evaluated over a sliding window using the `rolling` method.\n",
    "\n",
    "For example, to compute the 30-day moving average for Apple price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price[\"AAPL\"].rolling(30).mean().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, rolling functions require all of the values in the window to be non-NA. This behavior can be changed to account for missing data and, especially at the beginning of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price[\"AAPL\"].rolling(30, min_periods=3).mean().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Compute a 10-day moving average for `GOOG` with a min period of 5 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Covariance and Correlation\n",
    "\n",
    "Covariance and correlation measure the relationship between two variables.\n",
    "\n",
    "* **Covariance:** Measure of how much two random variables vary together. Hard to interpret magnitude.\n",
    "* **Correlation:** Normalized measure (-1 to 1). 1 is perfect positive correlation, -1 is perfect negative.\n",
    "\n",
    "In finance, we usually look at **Returns** (Percent Change), not raw prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = price.pct_change()\n",
    "returns.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the correlation and covariance between the returns of `MSFT` and `IBM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Covariance:\", returns[\"MSFT\"].cov(returns[\"IBM\"]))\n",
    "print(\"Correlation:\", returns[\"MSFT\"].corr(returns[\"IBM\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the full (pair-wise) correlation or covariance matrix as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compute pair-wise correlations between a DataFrameâ€™s columns or rows with another Series or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of all companies against IBM\n",
    "returns.corrwith(returns[\"IBM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of returns against volume\n",
    "returns.corrwith(volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Wrangling (Merge & Reshape)\n",
    "\n",
    "**Learning Objective:** Combine data from different sources (SQL-style Joins) and reshape table layouts (Wide to Long) to prepare for analysis.\n",
    "\n",
    "## 2.1 Merging (Joins)\n",
    "\n",
    "`merge` connects rows in DataFrames based on one or more keys. This is equivalent to database `join` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"], \n",
    "                    \"data1\": pd.Series(range(7), dtype=\"Int64\")})\n",
    "\n",
    "df2 = pd.DataFrame({\"key\": [\"a\", \"b\", \"d\"], \n",
    "                    \"data2\": pd.Series(range(3), dtype=\"Int64\")})\n",
    "\n",
    "print(\"DF1 (Left):\\n\", df1)\n",
    "print(\"\\nDF2 (Right):\\n\", df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many-to-One Join:** `df1` has multiple rows labeled `a` and `b`, whereas `df2` has only one row for each value in the key column `key`.\n",
    "\n",
    "The default is an **Inner Join** (intersection of keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to specify the key explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the column names are different in each object, you can specify them separately using `left_on` and `right_on`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\"lkey\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"], \n",
    "                    \"data1\": pd.Series(range(7), dtype=\"Int64\")})\n",
    "\n",
    "df4 = pd.DataFrame({\"rkey\": [\"a\", \"b\", \"d\"], \n",
    "                    \"data2\": pd.Series(range(3), dtype=\"Int64\")})\n",
    "\n",
    "pd.merge(df3, df4, left_on=\"lkey\", right_on=\"rkey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Types (Inner, Outer, Left, Right)\n",
    "\n",
    "You can specify the other options via the `how` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer Join: Union of keys. Fills missing with NaN\n",
    "pd.merge(df1, df2, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer Join with mismatched key names\n",
    "pd.merge(df3, df4, left_on=\"lkey\", right_on=\"rkey\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many-to-Many Join:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"], \n",
    "                    \"data1\": pd.Series(range(6), dtype=\"Int64\")})\n",
    "\n",
    "df2 = pd.DataFrame({\"key\": [\"a\", \"b\", \"a\", \"b\", \"d\"], \n",
    "                    \"data2\": pd.Series(range(5), dtype=\"Int64\")})\n",
    "\n",
    "pd.merge(df1, df2, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Merge `df1` and `df2` with a left join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Multiple Keys & Suffixes\n",
    "\n",
    "To merge with multiple keys, pass a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({\"key1\": [\"foo\", \"foo\", \"bar\"], \n",
    "                     \"key2\": [\"one\", \"two\", \"one\"],\n",
    "                     \"lval\": pd.Series([1, 2, 3], dtype='Int64')})\n",
    "\n",
    "right = pd.DataFrame({\"key1\": [\"foo\", \"foo\", \"bar\", \"bar\"],\n",
    "                      \"key2\": [\"one\", \"one\", \"one\", \"two\"],\n",
    "                      \"rval\": pd.Series([4, 5, 6, 7], dtype='Int64')})\n",
    "\n",
    "pd.merge(left, right, on=[\"key1\", \"key2\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are overlapping non-key column names, `merge` adds suffixes `_x` and `_y` by default. You can customize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"key1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"key1\", suffixes=(\"_left\", \"_right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on Index\n",
    "\n",
    "If the merge key(s) is in the index, you can pass `left_index=True` or `right_index=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left1 = pd.DataFrame({\"key\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"c\"],\n",
    "                      \"value\": pd.Series(range(6), dtype=\"Int64\")})\n",
    "\n",
    "right1 = pd.DataFrame({\"group_val\": [3.5, 7]}, index=[\"a\", \"b\"])\n",
    "\n",
    "pd.merge(left1, right1, left_on=\"key\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame has a `join` method which performs a left join by default. It's a convenient shortcut for index-on-index merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left1.join(right1, on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Reshaping and Pivoting\n",
    "\n",
    "We often need to switch between **Wide Format** (Excel style, years as columns) and **Long Format** (Database style, one row per observation).\n",
    "\n",
    "### Melt (Wide to Long)\n",
    "Let's look at our stock price data. It is currently **Wide**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index so Date is a column\n",
    "price_reindex = price.reset_index()\n",
    "price_reindex.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt into Long format\n",
    "melted = pd.melt(price_reindex, id_vars=\"Date\")\n",
    "melted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Rerun `melt` and pass arguments such that the new columns are named `Company` and `Price` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot (Long to Wide)\n",
    "Using `pivot`, we can reshape back to the original layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = melted.pivot(index='Date', columns='variable', values='value')\n",
    "reshaped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Aggregation & Reporting\n",
    "\n",
    "**Learning Objective:** Summarize data using GroupBy, Custom Aggregations, and Pivot Tables to answer business questions.\n",
    "\n",
    "## 3.1 Data Aggregation (GroupBy)\n",
    "\n",
    "Data aggregation is the process of grouping data together and performing calculations on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"key1\" : [\"a\", \"a\", None, \"b\", \"b\", \"a\", None], \n",
    "                   \"key2\" : pd.Series([1, 2, 1, 2, 1, None, 1], dtype=\"Int64\"),\n",
    "                   \"data1\" : np.random.standard_normal(7), \n",
    "                   \"data2\" : np.random.standard_normal(7)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute the mean for each unique value in `key1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make sense to compute the mean for `key2` since it is a categorical variable and also serves as a key.\n",
    "\n",
    "We can select the numeric columns to compute the mean for (after the `groupby` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\")[[\"data1\", \"data2\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following also works, since the returned result is a DataFrame, however it is less efficient as the selection/subset happens after the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\").mean()[[\"data1\", \"data2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can group by more than 1 column. There is a useful GroupBy method `size` which returns a Series containing group sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also group by other `Series`/`array`/`list` with the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([\"OH\", \"CA\", \"CA\", \"OH\", \"OH\", \"CA\", \"OH\"])\n",
    "years = [2005, 2005, 2006, 2005, 2006, 2005, 2006]\n",
    "\n",
    "df[\"data1\"].groupby([states, years]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Group by `key1` and `key2` and compute the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Custom Aggregation\n",
    "\n",
    "To use your own aggregation functions, pass any function that aggregates an array to the `aggregate` method or its short alias `agg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"key1\")\n",
    "grouped.agg(peak_to_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a list of functions, or function names (for built-in functions) to `aggregate`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg([peak_to_peak, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Pivot Tables\n",
    "\n",
    "Pivot tables are used to summarize, sort, reorganize, group, count, total or average data. It allows its users to transform columns into rows and rows into columns.\n",
    "\n",
    "We will use the `tips.csv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = pd.read_csv(\"../data/tips.csv\")\n",
    "\n",
    "# add a column with the tip percentage\n",
    "tips[\"tip_pct\"] = tips[\"tip\"] / tips[\"total_bill\"]\n",
    "\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default aggregation for `pivot_table` is mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"day\", \"smoker\"], values=[\"size\", \"tip\", \"tip_pct\", \"total_bill\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put `smoker` in the table columns and `time` and `day` in the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"day\"], columns=\"smoker\", \n",
    "                 values=[\"tip_pct\", \"size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add partial totals by passing `margins=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"day\"], columns=\"smoker\", \n",
    "                 values=[\"tip_pct\", \"size\"], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use other aggregation functions, pass it to the `aggfunc` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"smoker\"], columns=\"day\", \n",
    "                 values=\"tip_pct\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `fill_value` to fill missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"smoker\"], columns=\"day\", \n",
    "                 values=\"tip_pct\", aggfunc=len, margins=True, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Compute the sum of `tip` in a pivot table with `day` and `time` in the rows and `smoker` in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Tabulation\n",
    "\n",
    "A _cross-tabulation_ or _crosstab_ is a special case of pivot table that computes group frequencies (counts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=[tips[\"time\"], tips[\"day\"]], columns=tips[\"smoker\"], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Advanced Toolkit (Optional / Deep Dive)\n",
    "\n",
    "**Learning Objective:** Master complex data structures and advanced transformations. This section covers Hierarchical Indexing, Stacking, Concatenation, and custom Apply/Transform logic.\n",
    "\n",
    "## 4.1 Hierarchical Indexing (MultiIndex)\n",
    "\n",
    "Hierarchical indexing (MultiIndex) allows you to have multiple (two or more) _index levels_ on an axis. It enables \"higher dimensional\" data in a lower dimensional data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(np.random.uniform(size=9),\n",
    "                 index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],\n",
    "                 [1, 2, 3, 1, 3, 1, 2, 2, 3]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use _partial indexing_ to select subsets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"b\":\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[[\"b\", \"d\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select from \"inner\" level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexing works on both axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.arange(12).reshape((4, 3)),\n",
    "                        index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
    "                        columns=[['Ohio', 'Ohio', 'Colorado'],\n",
    "                        ['Green', 'Red', 'Green']])\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting names on the axes work as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.index.names = [\"key1\", \"key2\"]\n",
    "frame.columns.names = [\"state\", \"color\"]\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.index.nlevels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial indexing works on columns too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame[\"Ohio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reordering and Sorting Levels\n",
    "\n",
    "You may need to rearrange the order of the levels on an axis. The `swaplevel` method will swap the levels. The default is to swap the levels on the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.swaplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.swaplevel(0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sort by a single level or subset of levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.sort_index(level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Student Exercise:** Swap the levels on the rows then sort the index by level `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting and Resetting Index\n",
    "\n",
    "It's common to use one or more columns from a DataFrame as the row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({\"a\": range(7), \"b\": range(7, 0, -1), \n",
    "                      \"c\": [\"one\", \"one\", \"one\", \"two\", \"two\", \"two\", \"two\"], \n",
    "                      \"d\": [0, 1, 2, 0, 1, 2, 3]})\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_index` will return a new DataFrame using one or more of its columns as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2 = frame.set_index([\"c\", \"d\"])\n",
    "frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reset_index` does the opposite of `set_index` and turns the index back into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose to drop the columns when resetting index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Concatenation\n",
    "\n",
    "You can join DataFrames along any axis which is referred to as _concatenation_ or _stacking_. This is akin to database `union` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([0, 1], index=[\"a\", \"b\"], dtype=\"Int64\")\n",
    "s2 = pd.Series([2, 3, 4], index=[\"c\", \"d\", \"e\"], dtype=\"Int64\")\n",
    "s3 = pd.Series([5, 6], index=[\"f\", \"g\"], dtype=\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2, s3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `concat` works along `axis=\"index\"`, producing another Series. If you pass `axis=\"columns\"`, the result will instead be a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2, s3], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behavior of `concat` is union (`outer` join) of the indexes, you can also intersect them by passing `join='inner'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = pd.concat([s1, s3])\n",
    "pd.concat([s1, s4], axis=\"columns\", join=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combining Series along axis=\"columns\", pass the `keys` argument for the DataFrame column headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2, s3], axis=\"columns\", keys=[\"one\", \"two\", \"three\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Concat `s1`, `s2` and `s3` along index and pass `keys=[\"one\", \"two\", \"three\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the index does not contain any relevant data, and you want to avoid concatenating based on indexes, you can pass the `ignore_index=True` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.random.standard_normal((3, 4)), \n",
    "                   columns=[\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "df2 = pd.DataFrame(np.random.standard_normal((2, 3)), \n",
    "                   columns=[\"b\", \"d\", \"a\"])\n",
    "\n",
    "pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Stacking and Unstacking\n",
    "\n",
    "These are alternative reshaping methods to Melt/Pivot that work specifically on the Index levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.arange(6).reshape((2, 3)), \n",
    "                    index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n",
    "                    columns=pd.Index([\"one\", \"two\", \"three\"], name=\"number\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stack` method pivots the columns into rows, producing a Series with a MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.stack()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a hierarchically indexed Series, you can rearrange the data back into a DataFrame with `unstack` , which pivots rows into columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can unstack a different level by passing a level number or name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.unstack(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Advanced GroupBy: Apply\n",
    "\n",
    "The most general-purpose GroupBy method is `apply`, which splits the object being manipulated into pieces, invokes the passed function on each piece, and then concatenates the pieces.\n",
    "\n",
    "Suppose we want to select the top five `tip_pct` values by group. First, write a function that selects the rows with the largest values in a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(df, n=5, column=\"tip_pct\"):\n",
    "    return df.sort_values(column, ascending=False)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top(tips, n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then `apply` this function by different groups using `groupby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby(\"smoker\").apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass the arguments to the function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby([\"smoker\", \"day\"]).apply(top, n=2, column=\"total_bill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Exercise:** Apply the function on `day` and `time` group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Advanced GroupBy: Transform\n",
    "\n",
    "You can also transform your data using the `transform` method. It is similar to `apply` but the function must:\n",
    "- Produce a scalar value to be broadcast to the shape of the group chunk, or\n",
    "- Return an object that is the same shape as the group chunk\n",
    "\n",
    "This is useful for z-score normalization within groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['a', 'b', 'c'] * 4, 'value': np.arange(12.)})\n",
    "g = df.groupby('key')['value']\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform` produce a Series of the same shape as `df['value']` but with values replaced by the average grouped by `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.transform(lambda g: g.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "g.transform(normalize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}